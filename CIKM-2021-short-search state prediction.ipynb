{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "from collections import Counter\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: pre-computed feature file, should contain the label column as \"state\"\n",
    "df = pd.read_csv('./data/testing_input.csv') \n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "new_df = df.loc[:, 'QueryOrder':]\n",
    "labels = df['state'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 classes classifier\n",
    "\n",
    "def classify3(features, labels, clf_name, rand = 0):\n",
    "    performance_columns = ['random_state','clf_name', 'exploration_p', 'exploration_r', 'exploration_f1', \n",
    "                           'exploitation_p', 'exploitation_r', 'exploitation_f1', \n",
    "                           'knowitem_p', 'knowitem_r', 'knowitem_f1', \n",
    "                           'avg_p', 'avg_r', 'avg_f1', 'accu']\n",
    "#     clf_name = \"rf\"\n",
    "    if rand == 0:\n",
    "        rand_num = randrange(1000)\n",
    "    else:\n",
    "        rand_num = rand\n",
    "    \n",
    "    titles = list(features.columns)\n",
    "    feature_importances = pd.DataFrame(columns=titles)\n",
    "    fold_performances = pd.DataFrame(columns=performance_columns[1:])\n",
    "    kf = KFold(n_splits=10)#, shuffle=True, random_state = rand_num)\n",
    "    X, y = features, np.array(labels)\n",
    "    y_pred_all = []\n",
    "\n",
    "    if clf_name == 'nb':\n",
    "        clf = GaussianNB()\n",
    "    elif clf_name == 'dt':\n",
    "        clf = tree.DecisionTreeClassifier(random_state = rand_num)\n",
    "    elif clf_name == 'knn':\n",
    "        clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    elif clf_name == 'svm':\n",
    "        clf = svm.SVC()\n",
    "    elif clf_name == 'rf':\n",
    "        clf = RandomForestClassifier(random_state = rand_num)\n",
    "    elif clf_name == 'lr':\n",
    "        clf = LogisticRegression(random_state= rand_num)\n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        X_train, X_test= X.loc[train,:], X.loc[test,:]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        #importance\n",
    "        if clf_name == 'rf':\n",
    "            feature_importance = pd.DataFrame([clf.feature_importances_], index = [1], columns=titles)\n",
    "            feature_importances = feature_importances.append(feature_importance, ignore_index = True)\n",
    "        \n",
    "        #predict\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_all = y_pred_all + y_pred.tolist()\n",
    "        \n",
    "        fold_accu = accuracy_score(y_test, y_pred)\n",
    "        fold_1_f1, fold_2_f1, fold_3_f1=  f1_score(y_test, y_pred, average=None)\n",
    "        fold_1_p, fold_2_p, fold_3_p= precision_score(y_test, y_pred, average = None)\n",
    "        fold_1_r, fold_2_r, fold_3_r = recall_score(y_test, y_pred, average = None)\n",
    "        fold_avg_p = precision_score(y_test, y_pred, average = 'macro')\n",
    "        fold_avg_r = recall_score(y_test, y_pred, average = 'macro')\n",
    "        fold_avg_f1 =  f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        fold_performance = pd.DataFrame([[clf_name, fold_1_p, fold_1_r, fold_1_f1, fold_2_p, fold_2_r, fold_2_f1, \n",
    "                                   fold_3_p, fold_3_r, fold_3_f1, \n",
    "                                  fold_avg_p, fold_avg_r, fold_avg_f1, fold_accu]], \n",
    "                                        index = [fold_performances.shape[0]+1], columns=performance_columns[1:])\n",
    "        \n",
    "        fold_performances = fold_performances.append(fold_performance)\n",
    "    \n",
    "    accu = accuracy_score(y, y_pred_all)\n",
    "    exploration_f1, exploitation_f1, knownitem_f1 =  f1_score(y, y_pred_all, average=None)\n",
    "    exploration_p, exploitation_p, knownitem_p = precision_score(y, y_pred_all, average = None)\n",
    "    exploration_r, exploitation_r, knownitem_r = recall_score(y, y_pred_all, average = None)\n",
    "    avg_p = precision_score(y, y_pred_all, average = 'macro')\n",
    "    avg_r = recall_score(y, y_pred_all, average = 'macro')\n",
    "    avg_f1 =  f1_score(y, y_pred_all, average='macro')\n",
    "    \n",
    "    performance = pd.DataFrame([[rand_num, clf_name, exploration_p, exploration_r, exploration_f1, \n",
    "                                 exploitation_p, exploitation_r, exploitation_f1, \n",
    "                                 knownitem_p, knownitem_r, knownitem_f1,\n",
    "                    avg_p, avg_r, avg_f1, accu]], index = [1],columns=performance_columns)\n",
    "    \n",
    "    return performance, feature_importances, fold_performances, confusion_matrix(y, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances3, feature_importances3, fold_performances3, matrix = classify3(new_df, labels, 'rf')\n",
    "\n",
    "for clf in ['nb','lr','svm','knn','dt']:#,'mlp']:\n",
    "    print(matrix)\n",
    "    performance3, feature_importances3_tmp, fold_performances3, matrix = classify3(new_df, labels, clf)\n",
    "    performances3 = performances3.append(performance3,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performances3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_imp = feature_importances3.mean().sort_values(ascending=False)\n",
    "avg_imp.plot.bar(figsize=(20,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_imp.to_csv('./test_importance_output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
